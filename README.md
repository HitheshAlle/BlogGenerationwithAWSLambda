Serverless Blog Generation with AWS Bedrock, Lambda, and API Gateway
This project demonstrates how to build a serverless application on AWS that leverages Amazon Bedrock to generate blog posts on any given topic. The application is exposed via an API Gateway endpoint, which triggers a Lambda function to invoke the Llama 3 70B Instruct model. The generated blog is then automatically saved to an Amazon S3 bucket.
Project Architecture
The application follows a simple, event-driven, serverless architecture:
[Client (e.g., Postman)] --> [API Gateway (HTTP API)] --> [AWS Lambda Function] --> [Amazon Bedrock (Llama 3)]
                                       |
                                       '--> [Amazon S3 (Storage)]

1. A user sends a POST request with a blog topic to an API Gateway endpoint.
2. API Gateway triggers the AWS Lambda function, passing the topic.
3. The Lambda function invokes the Amazon Bedrock service, sending a prompt to the meta.llama3-70b-instruct-v1:0 model to generate a blog post.
4. Once the blog is generated by Bedrock, the Lambda function saves the content as a .txt file to a specified S3 bucket.
5. A success message is returned to the user.
Features
* Serverless: No servers to provision or manage. The application scales automatically with demand.
* Generative AI: Utilizes Amazon Bedrock and the powerful Llama 3 model for high-quality content generation.
* API-driven: Easily integrable with any frontend application or service through a simple REST API.
* Scalable Storage: Uses Amazon S3 for durable and scalable storage of generated content.
Prerequisites
Before you begin, ensure you have the following:
* An AWS Account with access to IAM, Lambda, API Gateway, S3, and Amazon Bedrock.
* Python 3.9+ installed on your local machine.
* The AWS CLI installed and configured.
* A tool for making API requests, such as Postman or curl.
Setup and Deployment
Follow these steps to deploy the application in your own AWS account.
1. Enable Model Access in Amazon Bedrock
First, you need to request access to the models you want to use.
* Navigate to the Amazon Bedrock console.
* In the bottom-left corner, click on Model access.
* Request access for the Meta Llama 3 models. You will need to wait for the access to be granted before proceeding.
2. Create an IAM User for Programmatic Access
It's a best practice to use an IAM user with specific permissions for interacting with AWS services programmatically.
* Navigate to the IAM console and go to Users. Click Create user.
* Give the user a name (e.g., bedrock-app-user) and select Provide user access to the AWS Management Console-optional.
* In the Set permissions step, choose Attach policies directly and attach the AdministratorAccess policy for simplicity in this demo. Note: For a production environment, you should create a more restrictive policy with only the necessary permissions (e.g., BedrockFullAccess, LambdaFullAccess, S3FullAccess, ApiGatewayAdministrator).
* Review and create the user.
* Once the user is created, go to the Security credentials tab and click Create access key.
* Choose Command Line Interface (CLI) as the use case, acknowledge the recommendation, and create the access key.
* Important: Download the .csv file or copy the Access key ID and Secret access key. You will need these to configure the AWS CLI.
3. Configure the AWS CLI
Open a terminal and configure the AWS CLI with the credentials of the IAM user you just created.
aws configure

Enter your Access Key ID, Secret Access Key, default region (e.g., us-east-1), and default output format (json).
4. Create the S3 Bucket
This bucket will store the generated blog posts.
* Navigate to the S3 console and click Create bucket.
* Enter a globally unique name for your bucket (e.g., aws-bedrock-blog-output-yourname).
* Keep the default settings and create the bucket.
5. Create the Lambda Function and Layer
The Lambda function contains the core logic, but since the default boto3 version in Lambda might not be up-to-date with Bedrock features, we will create a Lambda Layer with the latest version.
Step 5.1: Create the Lambda Layer
On your local machine, create a directory structure for the layer and install the latest boto3 library.
# Create directories
mkdir -p python/lib/python3.12/site-packages

# Install boto3 into the target directory
pip install boto3 -t python/lib/python3.12/site-packages

# Zip the python directory
zip -r boto3-layer.zip python

Now, create the layer in the Lambda console:
* Navigate to Lambda -> Layers and click Create layer.
* Give it a name (e.g., boto3_update_layer).
* Upload the boto3-layer.zip file you just created.
* Select the compatible runtimes (e.g., Python 3.12) and create the layer.
Step 5.2: Create the Lambda Function
* Navigate to the Lambda console and click Create function.
* Select Author from scratch.
* Enter a function name (e.g., blog-generator-function).
* Choose Python 3.12 as the runtime.
* Under Permissions, Lambda will create a new execution role. We will modify this role later.
* Create the function.
Step 5.3: Add Code and Attach the Layer
* In the function's Code source editor, replace the default lambda_function.py content with the code from the .py file provided in this repository.
* Scroll down to the Layers section and click Add a layer.
* Choose Custom layers, select the boto3_update_layer you created, and add it.
Step 5.4: Configure the Execution Role
The Lambda function needs permission to interact with Bedrock and S3.
* Go to the Configuration tab and click on Permissions.
* Click on the Role name to open the IAM role in a new tab.
* In the IAM role, click Add permissions -> Attach policies.
* Search for and attach the following AWS managed policies:
    * AmazonS3FullAccess
    * AmazonBedrockFullAccess
* Your Lambda function now has the necessary permissions.
6. Create the API Gateway
Finally, we'll create an HTTP API to expose our Lambda function to the internet.
* Navigate to the API Gateway console and click Create API.
* Choose to Build an HTTP API.
* Give your API a name (e.g., BedrockBlogAPI).
* In the Configure routes step, create a POST method with the route /blog-generation.
* In the Add integration step, select Lambda for the integration type and choose your blog-generator-function.
* Review and create the API.
* Once created, you will get an Invoke URL. This is your API endpoint.
Usage
You can now test the API using Postman or any other API client.
* Method: POST
* URL: Your API Gateway Invoke URL (e.g., https://xxxxxxxxx.execute-api.us-east-1.amazonaws.com/blog-generation)
* Body: Select raw and JSON. Provide the blog topic in the request body:
{
  "blog_topic": "The future of machine learning and generative AI"
}

* Click Send. You should receive a 200 OK response with the message: "Blog Generation is completed".
Verification
To verify that everything worked, navigate to your S3 bucket.
* You will see a folder named blog-output.
* Inside this folder, there will be a .txt file named with a timestamp (e.g., 184654.txt).
* Download and open this file to see the AI-generated blog post.
Code Explanation
lambda_function.py
This file contains three main functions:
1. blog_generate_using_bedrock(blogtopic:str) -> str:
    * Takes a blogtopic string as input.
    * Constructs a prompt specifically formatted for the Llama 3 instruct model.
    * Initializes the bedrock-runtime client using boto3. It includes a timeout and retry configuration for robustness.
    * Invokes the meta.llama3-70b-instruct-v1:0 model with the prompt and parameters (max_gen_len, temperature, top_p) to control the output.
    * Parses the streaming response from the model to extract the generated text.
    * Returns the blog content as a string.
2. save_blog_details_s3(s3_key, s3_bucket, generate_blog):
    * Takes the S3 key (file path), bucket name, and the blog content as input.
    * Initializes the S3 client.
    * Uses the put_object method to save the generate_blog content to the specified S3 bucket and key.
3. lambda_handler(event, context):
    * This is the main entry point for the Lambda function.
    * It parses the incoming event from API Gateway to get the blog_topic from the request body.
    * Calls blog_generate_using_bedrock() to generate the blog.
    * If the blog is generated successfully, it creates a unique S3 key using the current timestamp.
    * It then calls save_blog_details_s3() to store the file.
    * Returns a statusCode: 200 and a success message.
requirements.txt
The project requires the boto3 library, which is the AWS SDK for Python.
boto3
